

[
  
  
    
    
      {
        "title": "Hello World",
        "excerpt": "This is my very first blog post. I haven’t written anything yet but I’m sure I have some great stories to tell.\n",
        "content": "This is my very first blog post. I haven’t written anything yet but I’m sure I have some great stories to tell.\n",
        "url": "/general/2018/08/22/hello-world/"
      },
    
  
  
  
  {
    "title": "Join us",
    "excerpt": "\n",
    "content": "The Human and Machine Learning Lab is looking for exceptional postdoctoral fellows and Ph.D. students. NYU offers an extremely strong environment for training in computational cognitive science. Interested candidates can email Brenden Lake for additional details.\n\nPostdoctoral researchers\n\n  We are seeking a Postdoctoral researcher in self-supervised learning. Work with us to model child headcam videos, with the aim of learning knowledge of objects and agents from raw input. Deadline is Sept 1, 2020 or when position is filled. Apply here\n\n\nPh.D. candidates\n\n  Interested Ph.D. candidates are encouraged to apply to either the \nPh.D. Program in Data Science (deadline usually mid December) \nor the \nPh.D. Program in Cognition and Perception (deadline usually  December 1).\n  In exceptional cases it is possible to join the lab through the Ph.D. Program in Computer Science and the Ph.D. Program in Neural Science, but it is strongly recommended that you apply through either Data Science or Cognition and Perception.\n  University Policy states that you may only apply to one program at a time.\n\n",
    "url": "/apply/"
  },
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "Blog",
    "excerpt": "\n",
    "content": "\n",
    "url": "/blog/"
  },
  
  {
    "title": "People",
    "excerpt": "\n",
    "content": "Brenden Lake\nPrincipal Investigator\n\n\nBrenden is an Assistant Professor of Psychology and Data Science at New York University. He received his M.S. and B.S. in Symbolic Systems from Stanford University in 2009, and his Ph.D. in Cognitive Science from MIT in 2014. He was a postdoctoral Data Science Fellow at NYU from 2014-2017. Brenden is a recipient of the Robert J. Glushko Prize for Outstanding Doctoral Dissertation in Cognitive Science, he is a MIT Technology Review Innovator Under 35, and his research was selected by Scientific American as one of the most important advances of 2016. Brenden’s research focuses on computational problems that are easier for people than they are for machines, such as learning new concepts, creating new concepts, learning-to-learn, and asking questions.\n\nWebsite\n\nEmin Orhan\nPostdoc\n\n\nI obtained my PhD in Brain &amp; Cognitive Sciences from the University of Rochester. I was previously a postdoc at the Center for Neural Science at NYU and then jointly at Rice University &amp; Baylor College of Medicine. My research interests lie at the intersection of deep learning, cognitive science, and computational neuroscience. Broadly speaking, my research has three main goals: 1) Understanding how current deep learning models work, as well as characterizing their failure modes. 2) Comparing the behavior of deep learning models with qualitative and quantitative data from cognitive science and experimental neuroscience to better understand the mechanistic underpinnings of natural intelligence and to point out ways in which these models can be improved. 3) Based on the insights gleaned from the first two goals, improving the current generation of deep learning models.\n\nWebsite\n",
    "url": "/people/"
  },
  
  {
    "title": "Publications",
    "excerpt": "\n",
    "content": "A list of publications from HMLL\n\nPreprints\n\n\n  \n    Feinman, R. and Lake, B. M. (2020). Generating new concepts with hybrid neuro-symbolic models. Preprint available on arXiv:2003.08978.\n  \n  \n    Vong, W. K. and Lake, B. M. (2020). Learning word-referent mappings and concepts from raw inputs. Preprint available on arXiv:2003.05573.\n  \n  \n    Nye, M., Solar-Lezama, A., Tenenbaum, J. B., and Lake, B. M. (2020). Learning Compositional Rules via Neural Program Synthesis. Preprint available on arXiv:2003.05562.\n  \n  \n    Ruis, L., Andreas, J., Baroni, M. Bouchacourt, D., and Lake, B. M. (2020). A Benchmark for Systematic Generalization in Grounded Language Understanding. Preprint available on arXiv:2003.05161. [Benchmark] [Baseline model]\n  \n  \n    Davidson, G. and Lake, B. M. (2020). Investigating simple object representations in model-free deep reinforcement learning. Preprint available on arXiv:2002.06703.\n  \n  \n    Wang, Z. and Lake, B. M. (2019). Modeling question asking using neural program generation. Preprint available on arXiv:1907.09899.\n  \n  Gandhi, K. and Lake, B. M. (2019). Mutual exclusivity as a challenge for deep neural networks. Preprint available on arXiv:1906.10197.\n    \n      Press: New Scientist\n    \n  \n  Orhan, E. and Lake, B. M. (2019). Improving the robustness of ImageNet classifiers using elements of human visual cognition. Preprint available on arXiv:1906.08416.\n\n\n2020\n\n\n  \n    Lewis, M., Cristiano, V., Lake, B. M., Kwan, T., Frank, M. C. (2020). The role of developmental change and linguistic experience in the mutual exclusivity effect. Cognition, 198.\n  \n  \n    Lake, B. M. and Piantadosi, S. T. (2020). People infer recursive visual concepts from just a few examples. Computational Brain &amp; Behavior, 3(1), 54-65.\n[Supporting Info.]\n[Experiments]\n  \n\n\n2019\n\n\n  \n    Lake, B. M. (2019). Compositional generalization through meta sequence-to-sequence learning. Advances in Neural Information Processing Systems.\n[Code]\n  \n  \n    Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2019). The Omniglot challenge: a 3-year progress report. Current Opinion in Behavioral Sciences, 29, 97-104.\n  \n  \n    Feinman, R. and Lake, B. M. (2019). Learning a smooth kernel regularizer for convolutional neural networks. In Proceedings of the 41st Annual Conference of the Cognitive Science Society.\n  \n  \n    Lake, B. M., Linzen, T., and Baroni, M. (2019). Human few-shot learning of compositional instructions. In Proceedings of the 41st Annual Conference of the Cognitive Science Society.\n  \n  \n    Rothe, A., Lake, B. M., and Gureckis, T. M. (2019). Asking goal-oriented questions and learning from answers. In Proceedings of the 41st Annual Conference of the Cognitive Science Society.\n  \n\n\n2018\n\n\n  Rothe, A., Lake, B. M., and Gureckis, T. M. (2018). Do people ask good questions? Computational Brain &amp; Behavior, 1(1), 69-89.\n    \n      Outstanding paper award.\n    \n  \n  \n    Loula, J., Baroni, M., and Lake, B. M. (2018). Rearranging the familiar: Testing compositional generalization in recurrent networks. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP.\n  \n  \n    Lake, B. M. and Baroni, M. (2018). Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. International Conference on Machine Learning (ICML).\n[Supporting Info.]\n[Data set]\n  \n  \n    Feinman, R. and Lake, B. M. (2018). Learning inductive biases with simple neural networks. In Proceedings of the 40th Annual Conference of the Cognitive Science Society.\n  \n  Lake, B. M., Lawrence, N. D., and Tenenbaum, J. B. (2018). The emergence of organizing structure in conceptual representation. Cognitive Science, 42(S3), 809-832.\n[Supporting Info.]\n[Code]\n\n\n2017\n\n\n  \n    Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40, E253.\n  \n  \n    Rothe, A., Lake, B. M., and Gureckis, T. M. (2017). Question asking as program generation.  Advances in Neural Information Processing Systems 30.  [Supporting Info.]\n    \n      Press: MIT Technology Review\n    \n  \n\n\nSelect earlier papers\n\n\n  Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332-1338.\n[Supporting Info.]\n[visual Turing tests] \n[Omniglot data set] \n[Bayesian Program Learning code]\n    \n      Press: Science Podcast,\n            New York Times,\n            Reuters,\n            Popular Mechanics,\n            CBS,\n            MIT News,\n            MIT Technology Review,\n            Washington Post,\n            Toronto Star,\n            Scientific American “World Changing Idea”\n    \n  \n  \n    Lake, B. M., Zaremba, W., Fergus, R. and Gureckis, T. M. (2015). Deep Neural Networks Predict Category Typicality Ratings for Images. In Proceedings of the 37th Annual Conference of the Cognitive Science Society. [Data]\n  \n  Lake, B. M., Lee, C.-y., Glass, J. R., and Tenenbaum, J. B. (2014). One-shot learning of generative speech concepts. In Proceedings of the 36th Annual Conference of the Cognitive Science Society. [Supporting Info.]\n\n",
    "url": "/publications/"
  },
  
  {
    "title": "Research Statement",
    "excerpt": "\n",
    "content": "Informing A.I. with Human Intelligence\n\nOur lab is investigating the basic questions of intelligence. What makes people smarter than machines? What ingredients enable the fast and flexible ways in which humans learn? Can we build machines that learn and think in more human-like ways?\n\nIn the last few years, there has been remarkable advances in machine learning and artificial intelligence. Computers have beaten Jeopardy champions, defeated Go masters, driven autonomous cars, and shattered records for object and speech recognition. Progress has been remarkable, and yet, the best example of intelligence is still natural intelligence. Human minds solve a diverse array of difficult computational problems every day: concept learning, object recognition, scene understanding, language acquisition, speech recognition, question asking, amongst many others. Machines also struggle to simulate other facets of human intelligence, including creativity, general purpose problem solving, and commonsense reasoning.\n\nOur lab builds computational models of everyday cognitive abilities, focusing on computational problems that are easier for people than they are for machines. Almost by definition, these problems are interesting scientific pursuits for both cognitive science and machine learning. In cognitive science, if people outperform all existing algorithms on certain types of problems, we must learn how people solve them. In machine learning, these cognitive abilities are both important open problems as well as opportunities to reverse engineer the human solutions.\n\nIn this broad space of computational challenges, our work has touched on questions such as: How do people learn a new concept from just one or a few examples? How do people act creatively when designing new concepts? How do people learn qualitatively different forms of structure? How do people ask questions when searching for information? To get at these questions, we use computational modeling and behavioral experiments; we are also exploring neuroimaging and developmental studies. Across this range of questions and techniques, our work has revealed key cognitive ingredients that people use but are missing in contemporary machine learning. It has also led to new machine learning and data science techniques inspired by the cognitive solutions to these difficult computational problems.\n",
    "url": "/research/"
  },
  
  {
    "title": "Resources",
    "excerpt": "\n",
    "content": "Data and code from HMLL\n\nOmniglot Data Set\n\n\nThe Omniglot dataset contains over 1600 handwritten characters from 50 different alphabets. It’s used for studying one-shot learning and for developing more human-like learning algorithms.\n\nGitHub link\n\nSCAN Data Set\n\n\nSCAN is a set of simple language-driven navigation tasks for studying compositional learning and zero-shot generalization.\n\nGitHub link\n",
    "url": "/resources/"
  }
  
]

